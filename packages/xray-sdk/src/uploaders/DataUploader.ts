/**
 * DataUploader - Handles data blob uploads (immediate, non-batched)
 * Serializes data via worker pool, writes to storage, uploads to S3 with chunking
 */

import { StorageAdapter } from "../storage/StorageAdapter";
import { WorkerPool } from "../workers/WorkerPool";
import { ApiClient } from "../api";
import { PresignRequest } from "../types";
import { DebugLogger } from "../utils/debug";

interface UploadTask {
  dataId: string;
  traceId: string;
  key: string;
  data: unknown;
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  metadata?: Record<string, any>;
}

const CHUNK_SIZE = 10 * 1024 * 1024; // 10MB chunks

/**
 * DataUploader handles immediate upload of data blobs with chunking support
 */
export class DataUploader {
  private storage: StorageAdapter;
  private workerPool: WorkerPool;
  private apiClient: ApiClient;
  private debug: DebugLogger;
  private uploadTasks: Map<string, Promise<void>> = new Map();

  constructor(
    storage: StorageAdapter,
    workerPool: WorkerPool,
    apiClient: ApiClient,
    debug: boolean = false
  ) {
    this.storage = storage;
    this.workerPool = workerPool;
    this.apiClient = apiClient;
    this.debug = new DebugLogger(debug);
  }

  /**
   * Upload data blob (fire-and-forget, returns immediately)
   * dataId is generated by caller, upload happens in background
   */
  async upload(
    traceId: string,
    data: unknown,
    key: string,
    dataId: string,
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    metadata?: Record<string, any>
  ): Promise<void> {
    this.debug.log(
      `[DataUploader] upload() started: dataId=${dataId}, traceId=${traceId}, key=${key}`
    );

    // Start upload in background (fire-and-forget)
    const uploadPromise = this.doUpload({
      dataId,
      traceId,
      key,
      data,
      metadata,
    }).then(() => {
      this.debug.log(`[DataUploader] upload() completed: dataId=${dataId}`);
    });

    this.uploadTasks.set(dataId, uploadPromise);

    // Clean up promise after completion
    uploadPromise
      .catch(() => {
        // Errors handled internally, promise cleanup only
      })
      .finally(() => {
        this.uploadTasks.delete(dataId);
      });
  }

  /**
   * Internal upload method with retry logic
   */
  private async doUpload(task: UploadTask): Promise<void> {
    const maxAttempts = 5;
    const maxDelay = 10000; // 10 seconds

    for (let attempt = 1; attempt <= maxAttempts; attempt++) {
      try {
        await this.attemptUpload(task);
        return; // Success
      } catch (error) {
        if (attempt === maxAttempts) {
          // Last attempt failed, log and exit
          this.debug.warn(
            `[DataUploader] Upload failed after ${maxAttempts} attempts:`,
            error
          );
          return; // Silent failure - don't throw
        }

        // Calculate delay with exponential backoff and jitter
        const baseDelay = Math.min(1000 * Math.pow(2, attempt - 1), maxDelay);
        const jitter = Math.random() * 100; // 0-100ms jitter
        const delay = baseDelay + jitter;

        this.debug.warn(
          `[DataUploader] Upload attempt ${attempt} failed, retrying in ${delay.toFixed(
            0
          )}ms:`,
          error
        );

        await new Promise((resolve) => setTimeout(resolve, delay));
      }
    }
  }

  /**
   * Single upload attempt with chunking support
   */
  private async attemptUpload(task: UploadTask): Promise<void> {
    // 1. Serialize data using worker pool
    const buffer = await this.workerPool.serialize(task.data);

    // 2. Write serialized data to local storage
    const storageId = `data-${task.dataId}`;
    await this.storage.write(storageId, buffer, "data");

    // 3. Request presigned URL from backend
    const presignRequest: PresignRequest = {
      dataId: task.dataId,
      traceId: task.traceId,
      key: task.key,
      metadata: task.metadata,
    };

    const presignResponse = await this.apiClient.presign(presignRequest);

    // 4. Upload to S3 (with chunking if needed)
    if (buffer.length <= CHUNK_SIZE) {
      // Small file: single PUT upload
      await this.uploadSingle(presignResponse.presignedUrl, buffer);
    } else {
      // Large file: multipart upload with chunks
      await this.uploadMultipart(
        presignResponse.presignedUrl,
        buffer,
        task.dataId
      );
    }

    // 5. Delete local file on success
    await this.storage.delete(storageId);
  }

  /**
   * Upload small file using single PUT request
   */
  private async uploadSingle(
    presignedUrl: string,
    buffer: Buffer
  ): Promise<void> {
    const response = await fetch(presignedUrl, {
      method: "PUT",
      headers: {
        "Content-Type": "application/json",
      },
      body: buffer,
    });

    if (!response.ok) {
      throw new Error(
        `S3 upload failed: ${response.status} ${response.statusText}`
      );
    }
  }

  /**
   * Upload large file using S3 multipart upload with chunks
   */
  private async uploadMultipart(
    basePresignedUrl: string,
    buffer: Buffer,
    dataId: string
  ): Promise<void> {
    // Initiate multipart upload
    const initiateUrl = new URL(basePresignedUrl);
    initiateUrl.searchParams.set("uploads", "");
    const initiateResponse = await fetch(initiateUrl.toString(), {
      method: "POST",
    });

    if (!initiateResponse.ok) {
      throw new Error(
        `Failed to initiate multipart upload: ${initiateResponse.status} ${initiateResponse.statusText}`
      );
    }

    const initiateXml = await initiateResponse.text();
    const uploadIdMatch = initiateXml.match(/<UploadId>([^<]+)<\/UploadId>/);
    if (!uploadIdMatch) {
      throw new Error(
        "Failed to extract uploadId from multipart upload initiation"
      );
    }
    const uploadId = uploadIdMatch[1];

    try {
      // Upload chunks
      const parts: Array<{ partNumber: number; etag: string }> = [];
      const totalParts = Math.ceil(buffer.length / CHUNK_SIZE);

      for (let partNumber = 1; partNumber <= totalParts; partNumber++) {
        const start = (partNumber - 1) * CHUNK_SIZE;
        const end = Math.min(start + CHUNK_SIZE, buffer.length);
        const chunk = buffer.subarray(start, end);

        const partUrl = new URL(basePresignedUrl);
        partUrl.searchParams.set("partNumber", partNumber.toString());
        partUrl.searchParams.set("uploadId", uploadId);

        const partResponse = await fetch(partUrl.toString(), {
          method: "PUT",
          headers: {
            "Content-Type": "application/json",
          },
          body: chunk,
        });

        if (!partResponse.ok) {
          throw new Error(
            `Failed to upload part ${partNumber}: ${partResponse.status} ${partResponse.statusText}`
          );
        }

        const etag = partResponse.headers.get("etag");
        if (!etag) {
          throw new Error(`Failed to get ETag for part ${partNumber}`);
        }

        parts.push({ partNumber, etag });

        this.debug.log(
          `[DataUploader] Uploaded part ${partNumber}/${totalParts} for ${dataId}`
        );
      }

      // Complete multipart upload
      const completeUrl = new URL(basePresignedUrl);
      completeUrl.searchParams.set("uploadId", uploadId);

      // Build XML for complete request
      const completeXml = `<?xml version="1.0" encoding="UTF-8"?>
<CompleteMultipartUpload>
${parts
  .map(
    (p) => `  <Part>
    <PartNumber>${p.partNumber}</PartNumber>
    <ETag>${p.etag}</ETag>
  </Part>`
  )
  .join("\n")}
</CompleteMultipartUpload>`;

      const completeResponse = await fetch(completeUrl.toString(), {
        method: "POST",
        headers: {
          "Content-Type": "application/xml",
        },
        body: completeXml,
      });

      if (!completeResponse.ok) {
        throw new Error(
          `Failed to complete multipart upload: ${completeResponse.status} ${completeResponse.statusText}`
        );
      }
    } catch (error) {
      // Abort multipart upload on error
      try {
        const abortUrl = new URL(basePresignedUrl);
        abortUrl.searchParams.set("uploadId", uploadId);
        await fetch(abortUrl.toString(), {
          method: "DELETE",
        });
      } catch (abortError) {
        // Ignore abort errors, original error is more important
        this.debug.warn(
          "[DataUploader] Failed to abort multipart upload:",
          abortError
        );
      }
      throw error;
    }
  }

  /**
   * Wait for all pending uploads to complete (for testing/shutdown)
   */
  async waitForPendingUploads(): Promise<void> {
    const promises = Array.from(this.uploadTasks.values());
    await Promise.allSettled(promises);
  }
}
